{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_meaning = {1, 2, 7}\n",
    "# this word can refer to entities 1, 2, and 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Shorthair:\n",
    "    def __init__(self, name):\n",
    "        self.name = name\n",
    "        self.personality = 'strange'\n",
    "\n",
    "    def speak(self):\n",
    "        print(self.name + ' goes, \"Meow.\"')\n",
    "\n",
    "Sauce = Shorthair('Saucy')\n",
    "Ketchup = Shorthair('Ketchup')\n",
    "Mayo = Shorthair('Mayo')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat = {Sauce, Ketchup, Mayo}\n",
    "# cat is a noun, and it can refer to the shorthair cats, Sauce, Ketchup, and Mayo\n",
    "# for this example, the set of possible entities that cat can refer to is Sauce, Ketchup, and Mayo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{<__main__.Shorthair at 0x10a170dd0>,\n",
       " <__main__.Shorthair at 0x114ab72d0>,\n",
       " <__main__.Shorthair at 0x114ab7f50>}"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cat = {Sauce, Ketchup, Mayo, Mayo}\n",
    "cat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/g_/v_4svj7n17z2m3fg9lhk_xcw0000gp/T/ipykernel_29152/1457701759.py:6: DeprecationWarning: `set_matplotlib_formats` is deprecated since IPython 7.23, directly use `matplotlib_inline.backend_inline.set_matplotlib_formats()`\n",
      "  set_matplotlib_formats('svg', 'pdf')\n"
     ]
    }
   ],
   "source": [
    "from numpy import prod\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import set_matplotlib_formats\n",
    "set_matplotlib_formats('svg', 'pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_probabilities(probs):\n",
    "    total = sum(probs)\n",
    "    normalized_probs = []\n",
    "    for p in probs:\n",
    "        normalized_probs.append(p / total)\n",
    "    return normalized_probs\n",
    "\n",
    "# sum every probability in a list of probs, e.g. [0.2, 0.3, 0.4, 0.5] \n",
    "# for every probability in that list, divide it by the calculated sum\n",
    "# 0.2 / (0.2 + 0.3 + 0.4 + 0.5) = 0.2 / 0.9 = 0.2 repeating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "# learners consider hypotheses of word meanings\n",
    "\n",
    "class Longhair:\n",
    "    def __init__(self, name):\n",
    "        self.name = name\n",
    "        self.personality = 'fanciful'\n",
    "\n",
    "Hiccup = Longhair('Hiccup')\n",
    "\n",
    "cat_hypothesis_space = [{set[Shorthair]}, {set[Shorthair], set[Longhair]}]\n",
    "# the word cat can refer to the set of Shorthairs, OR the set of shorthairs and the set of longhairs, that includes Hiccup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{0},\n",
       " {1},\n",
       " {2},\n",
       " {3},\n",
       " {4},\n",
       " {5},\n",
       " {6},\n",
       " {7},\n",
       " {8},\n",
       " {9},\n",
       " {10},\n",
       " {0, 1},\n",
       " {1, 2},\n",
       " {2, 3},\n",
       " {3, 4},\n",
       " {4, 5},\n",
       " {5, 6},\n",
       " {6, 7},\n",
       " {7, 8},\n",
       " {8, 9},\n",
       " {9, 10},\n",
       " {0, 1, 2},\n",
       " {1, 2, 3},\n",
       " {2, 3, 4},\n",
       " {3, 4, 5},\n",
       " {4, 5, 6},\n",
       " {5, 6, 7},\n",
       " {6, 7, 8},\n",
       " {7, 8, 9},\n",
       " {8, 9, 10},\n",
       " {0, 1, 2, 3},\n",
       " {1, 2, 3, 4},\n",
       " {2, 3, 4, 5},\n",
       " {3, 4, 5, 6},\n",
       " {4, 5, 6, 7},\n",
       " {5, 6, 7, 8},\n",
       " {6, 7, 8, 9},\n",
       " {7, 8, 9, 10},\n",
       " {0, 1, 2, 3, 4},\n",
       " {1, 2, 3, 4, 5},\n",
       " {2, 3, 4, 5, 6},\n",
       " {3, 4, 5, 6, 7},\n",
       " {4, 5, 6, 7, 8},\n",
       " {5, 6, 7, 8, 9},\n",
       " {6, 7, 8, 9, 10},\n",
       " {0, 1, 2, 3, 4, 5},\n",
       " {1, 2, 3, 4, 5, 6},\n",
       " {2, 3, 4, 5, 6, 7},\n",
       " {3, 4, 5, 6, 7, 8},\n",
       " {4, 5, 6, 7, 8, 9},\n",
       " {5, 6, 7, 8, 9, 10}]"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_hypotheses = [{0},{1},{2},{3},{4},{5},{6},{7},{8},{9},{10},\n",
    "         {0,1},{1,2},{2,3},{3,4},{4,5},{5,6},{6,7},{7,8},{8,9},{9,10},\n",
    "         {0,1,2},{1,2,3},{2,3,4},{3,4,5},{4,5,6},{5,6,7},{6,7,8},{7,8,9},{8,9,10},\n",
    "         {0,1,2,3},{1,2,3,4},{2,3,4,5},{3,4,5,6},{4,5,6,7},{5,6,7,8},{6,7,8,9},{7,8,9,10},\n",
    "         {0,1,2,3,4},{1,2,3,4,5},{2,3,4,5,6},{3,4,5,6,7},{4,5,6,7,8},{5,6,7,8,9},{6,7,8,9,10},\n",
    "         {0,1,2,3,4,5},{1,2,3,4,5,6},{2,3,4,5,6,7},{3,4,5,6,7,8},{4,5,6,7,8,9},{5,6,7,8,9,10}]\n",
    "\n",
    "all_hypotheses\n",
    "# assumptions:\n",
    "# 1. there are 10 entities in this world, 0 - 10\n",
    "# 2. a word refers to at least one of those entities, and up to (inclusive) 6 entities\n",
    "# 3. hypotheses include entities that are clustered; in other words, 0 is clustered with entities 1, 2, 3, 4, 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_prior(possible_hypotheses):\n",
    "    prior = []\n",
    "    for h in possible_hypotheses:\n",
    "        prior.append(1/len(possible_hypotheses))\n",
    "    return prior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.0196078431372549,\n",
       " 0.0196078431372549,\n",
       " 0.0196078431372549,\n",
       " 0.0196078431372549,\n",
       " 0.0196078431372549,\n",
       " 0.0196078431372549,\n",
       " 0.0196078431372549,\n",
       " 0.0196078431372549,\n",
       " 0.0196078431372549,\n",
       " 0.0196078431372549,\n",
       " 0.0196078431372549,\n",
       " 0.0196078431372549,\n",
       " 0.0196078431372549,\n",
       " 0.0196078431372549,\n",
       " 0.0196078431372549,\n",
       " 0.0196078431372549,\n",
       " 0.0196078431372549,\n",
       " 0.0196078431372549,\n",
       " 0.0196078431372549,\n",
       " 0.0196078431372549,\n",
       " 0.0196078431372549,\n",
       " 0.0196078431372549,\n",
       " 0.0196078431372549,\n",
       " 0.0196078431372549,\n",
       " 0.0196078431372549,\n",
       " 0.0196078431372549,\n",
       " 0.0196078431372549,\n",
       " 0.0196078431372549,\n",
       " 0.0196078431372549,\n",
       " 0.0196078431372549,\n",
       " 0.0196078431372549,\n",
       " 0.0196078431372549,\n",
       " 0.0196078431372549,\n",
       " 0.0196078431372549,\n",
       " 0.0196078431372549,\n",
       " 0.0196078431372549,\n",
       " 0.0196078431372549,\n",
       " 0.0196078431372549,\n",
       " 0.0196078431372549,\n",
       " 0.0196078431372549,\n",
       " 0.0196078431372549,\n",
       " 0.0196078431372549,\n",
       " 0.0196078431372549,\n",
       " 0.0196078431372549,\n",
       " 0.0196078431372549,\n",
       " 0.0196078431372549,\n",
       " 0.0196078431372549,\n",
       " 0.0196078431372549,\n",
       " 0.0196078431372549,\n",
       " 0.0196078431372549,\n",
       " 0.0196078431372549]"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calculate_prior(all_hypotheses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "input = [7, 7, 7, 7, 8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "def likelihood(data, hypothesis):\n",
    "    likelihoods = []\n",
    "    for data_item in data:\n",
    "        if data_item in hypothesis:\n",
    "            likelihood_this_item = 1/len(hypothesis)\n",
    "        else: \n",
    "            likelihood_this_item = 0\n",
    "        likelihoods.append(likelihood_this_item)\n",
    "    return prod(likelihoods)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n",
      "0.0\n",
      "0.0\n"
     ]
    }
   ],
   "source": [
    "print(likelihood([0, 6, 7, 8], {7, 8, 9}))\n",
    "print(likelihood([6, 7], {7, 8, 9}))\n",
    "print(likelihood([6, 7], {7, 8, 9}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "def posterior(data, possible_hypotheses, prior):\n",
    "    posteriors = []\n",
    "    for i in range(len(possible_hypotheses)):\n",
    "        h = possible_hypotheses[i]\n",
    "        prior_h = prior[i]\n",
    "        likelihood_h = likelihood(data, h)\n",
    "        posterior_h = prior_h * likelihood_h\n",
    "        posteriors.append(posterior_h)\n",
    "    return normalize_probabilities(posteriors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.6568863586599518,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.19463299515850427,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.08211079483249398,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.042040726954236926,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.024329124394813034,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0]"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_prior = calculate_prior(all_hypotheses)\n",
    "posterior([0, 0, 1], all_hypotheses, my_prior)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Questions:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. How does the amount of data influence the posterior distribution? For instance, is the posterior the same after seeing the data [0, 0, 1] and the data [0, 0, 1, 0, 0, 1]? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0} 0.0\n",
      "{1} 0.0\n",
      "{2} 0.0\n",
      "{3} 0.0\n",
      "{4} 0.0\n",
      "{5} 0.0\n",
      "{6} 0.0\n",
      "{7} 0.0\n",
      "{8} 0.0\n",
      "{9} 0.0\n",
      "{10} 0.0\n",
      "{0, 1} 0.6568863586599518\n",
      "{1, 2} 0.0\n",
      "{2, 3} 0.0\n",
      "{3, 4} 0.0\n",
      "{4, 5} 0.0\n",
      "{5, 6} 0.0\n",
      "{6, 7} 0.0\n",
      "{8, 7} 0.0\n",
      "{8, 9} 0.0\n",
      "{9, 10} 0.0\n",
      "{0, 1, 2} 0.19463299515850427\n",
      "{1, 2, 3} 0.0\n",
      "{2, 3, 4} 0.0\n",
      "{3, 4, 5} 0.0\n",
      "{4, 5, 6} 0.0\n",
      "{5, 6, 7} 0.0\n",
      "{8, 6, 7} 0.0\n",
      "{8, 9, 7} 0.0\n",
      "{8, 9, 10} 0.0\n",
      "{0, 1, 2, 3} 0.08211079483249398\n",
      "{1, 2, 3, 4} 0.0\n",
      "{2, 3, 4, 5} 0.0\n",
      "{3, 4, 5, 6} 0.0\n",
      "{4, 5, 6, 7} 0.0\n",
      "{8, 5, 6, 7} 0.0\n",
      "{8, 9, 6, 7} 0.0\n",
      "{8, 9, 10, 7} 0.0\n",
      "{0, 1, 2, 3, 4} 0.042040726954236926\n",
      "{1, 2, 3, 4, 5} 0.0\n",
      "{2, 3, 4, 5, 6} 0.0\n",
      "{3, 4, 5, 6, 7} 0.0\n",
      "{4, 5, 6, 7, 8} 0.0\n",
      "{5, 6, 7, 8, 9} 0.0\n",
      "{6, 7, 8, 9, 10} 0.0\n",
      "{0, 1, 2, 3, 4, 5} 0.024329124394813034\n",
      "{1, 2, 3, 4, 5, 6} 0.0\n",
      "{2, 3, 4, 5, 6, 7} 0.0\n",
      "{3, 4, 5, 6, 7, 8} 0.0\n",
      "{4, 5, 6, 7, 8, 9} 0.0\n",
      "{5, 6, 7, 8, 9, 10} 0.0\n"
     ]
    }
   ],
   "source": [
    "my_posterior1 = posterior([0, 0, 1], all_hypotheses, my_prior)\n",
    "for i in range(len(all_hypotheses)):\n",
    "    print(all_hypotheses[i], my_posterior1[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0} 0.0\n",
      "{1} 0.0\n",
      "{2} 0.0\n",
      "{3} 0.0\n",
      "{4} 0.0\n",
      "{5} 0.0\n",
      "{6} 0.0\n",
      "{7} 0.0\n",
      "{8} 0.0\n",
      "{9} 0.0\n",
      "{10} 0.0\n",
      "{0, 1} 0.9018073901245205\n",
      "{1, 2} 0.0\n",
      "{2, 3} 0.0\n",
      "{3, 4} 0.0\n",
      "{4, 5} 0.0\n",
      "{5, 6} 0.0\n",
      "{6, 7} 0.0\n",
      "{8, 7} 0.0\n",
      "{8, 9} 0.0\n",
      "{9, 10} 0.0\n",
      "{0, 1, 2} 0.07917101916045172\n",
      "{1, 2, 3} 0.0\n",
      "{2, 3, 4} 0.0\n",
      "{3, 4, 5} 0.0\n",
      "{4, 5, 6} 0.0\n",
      "{5, 6, 7} 0.0\n",
      "{8, 6, 7} 0.0\n",
      "{8, 9, 7} 0.0\n",
      "{8, 9, 10} 0.0\n",
      "{0, 1, 2, 3} 0.014090740470695633\n",
      "{1, 2, 3, 4} 0.0\n",
      "{2, 3, 4, 5} 0.0\n",
      "{3, 4, 5, 6} 0.0\n",
      "{4, 5, 6, 7} 0.0\n",
      "{8, 5, 6, 7} 0.0\n",
      "{8, 9, 6, 7} 0.0\n",
      "{8, 9, 10, 7} 0.0\n",
      "{0, 1, 2, 3, 4} 0.0036938030699500374\n",
      "{1, 2, 3, 4, 5} 0.0\n",
      "{2, 3, 4, 5, 6} 0.0\n",
      "{3, 4, 5, 6, 7} 0.0\n",
      "{4, 5, 6, 7, 8} 0.0\n",
      "{5, 6, 7, 8, 9} 0.0\n",
      "{6, 7, 8, 9, 10} 0.0\n",
      "{0, 1, 2, 3, 4, 5} 0.001237047174382058\n",
      "{1, 2, 3, 4, 5, 6} 0.0\n",
      "{2, 3, 4, 5, 6, 7} 0.0\n",
      "{3, 4, 5, 6, 7, 8} 0.0\n",
      "{4, 5, 6, 7, 8, 9} 0.0\n",
      "{5, 6, 7, 8, 9, 10} 0.0\n"
     ]
    }
   ],
   "source": [
    "my_posterior2 = posterior([0, 0, 1, 0, 0, 1], all_hypotheses, my_prior)\n",
    "for i in range(len(all_hypotheses)):\n",
    "    print(all_hypotheses[i], my_posterior2[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After seeing the data, [0, 0, 1], the hypothesis {0, 1} has a posterior probability of 0.6568863586599518. After seeing the data, [0, 0, 1, 0, 0, 1], the hypothesis {0, 1} has a posterior probability of 0.9018073901245205.\n",
    "The posterior probability of hypothesis {0, 1} increases after more information (i.e., data) is seen.\n",
    "The posterior probability of hypothesis {0, 1, 2} decreases after more information: after [0, 0, 1] it is 0.19463299515850427, and after [0, 0, 1, 0, 0, 1] it is 0.07917101916045172.\n",
    "In effect, the hypothesis that predicts fewer word meanings is preferred after more data.\n",
    "This reflects the size principle: \n",
    "\n",
    "$p(X|h)=[\\frac{1}{size(h)}]^n$\n",
    "\n",
    "Hypotheses that predict the word has fewer meanings (i.e., that have smaller extensions), assign exponentially greater probability to the same data than do hypotheses that predict the word has many more meanings. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. When are more specific word meanings preferred? When are more general word meanings preferred?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The most specific word meanings that explain (or, predict) the observed data are always preferred: they have fewer extensions, and if these extensions occur in the observed data, they are preferred over hypotheses that maintain extensions that did not occur in the observed data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. This code calculates a probability distribution over possible hypotheses given some data. If you had to commit to a single hypothesis, how would you choose one?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I would choose one that explains the data best. Given that I have no (known) preference for any of the possible hypotheses (in other words, equal priors assigned to each hypothesis), I would pick the hypothesis with the greatest likelihood: given the data, the hypothesis that best predicted the observed data. \n",
    "\n",
    "In this case, I pick the hypothesis with the greatest posterior probability. I pick the MAP hypothesis. \n",
    "\n",
    "If I do not pick the MAP hypothesis, I could pick a random hypothesis, despite the fact that there is one specific hypothesis with the greatest MAP. However, I would be more likely to select hypotheses that have greater posterior probabilities. In effect, I would more often select from hypotheses with posterior probabilities of 0.8, 0.9, etc, but I could select one with a low posterior probability."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Do we have any kind of innateness in our model? Are there word meanings that our model learner could never learn, no matter what kind of data we gave them?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If innateness means what the learner brings to the task of learning word meanings, "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
